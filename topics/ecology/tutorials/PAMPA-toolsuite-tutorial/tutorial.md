---
layout: tutorial_hands_on

title: Compute and analyse Essential Biodiversity Variables with PAMPA toolsuite
zenodo_link: ''
questions:
- Which biological questions are addressed by the tutorial?
- Which bioinformatics techniques are important to know for this type of data?
objectives:
- The learning objectives are the goals of the tutorial
- They will be informed by your audience and will communicate to them and to yourself
  what you should focus on during the course
- They are single sentences describing what a learner should be able to do once they
  have completed the tutorial
- You can use Bloom's Taxonomy to write effective learning objectives
time_estimation: 3H
key_points:
- The take-home messages
- They will appear at the end of the tutorial
contributors:
- ColineRoyaux

---


# Introduction
{:.no_toc}


This tutorial aims to present the PAMPA Galaxy workflow and how to use it to compute 
Essential Biodiversity Variables (EBV) from species abundance data and analyse it through generalized 
linear (mixed) models (GLM and GLMM). This workflow made up of 5 tools will allow you to process
temporal series data that include at least year, location and species sampled along with 
abundance value and generate article-ready data products.

The PAMPA workflow is an EBV workflow, like so, it is divided as the EBV classes ```Community``` and 
```Species-Population```. Only the last tool to create a plot from your GLM results is common to the 
two parts of this workflow. Thus, in this tutorial, we'll start by the ```Community``` analysis followed
by the ```Species-Population``` analysis.

> ### {% icon details %} Ecological definitions of ```Population``` and ```Community```
> 
> ```Population``` : group of individuals of the same species interacting with each other.
> ```Community``` : group of several species interacting with each other. 
> 
{: .details}

![PAMPA toolsuite workflow](../../images/PAMPA_toolsuite_tutorial/PAMPA_workflow.png "PAMPA toolsuite workflow")

Each part of this workflow has the same elementary steps :
 - **A first tool** to compute metrics : 
   - ```Community``` metrics (**Calculate community metrics** {% icon tool %}) : Total abundance, Species richness, 
     Shannon index, Simpson index, Hill index and Pielou index
   - ```Species-Population``` metrics (**Calculate presence absence table** {% icon tool %}) : Abundance, Presence-absence
 - **A second tool** to compute Generalized Linear (Mixed) Models, testing the effect of ```site```, ```year``` and/or 
   ```habitat``` on a metric selected by the user : 
   - ```Community``` analysis (**Compute GLM on community data** {% icon tool %}) : The GLM(M) is computed on the whole 
     dataset and, if a separation factor is selected by the user, GLM(M)s are computed on subsets of the dataset 
     (example : one GLM(M) per ecoregion)
   - ```Species-Population``` analysis (**Compute GLM on population data** {% icon tool %}) : One GLM(M) is computed 
     on each species separately
 - **A third tool** common for ```Community``` and ```Species-Population``` analyses with ```year``` as a fixed effect to 
   create time-series plots from your GLM(M) results (**Create a plot from GLM data** {% icon tool %}). Two plots will be created for each GLM(M) : one from the 'Estimate'
   values of the GLM(M) and one from raw values from metrics file.

> ### {% icon details %} Details about Generalized Linear (Mixed) Models
> 
> Generalized Linear Models (GLMs) and Generalized Linear Mixed Models (GLMMs) are two extensions of Linear Models (LMs).
> As LMs uses only Gaussian distribution (for continuous variables), GLMs permits to analyse a wide spectrum of data 
> with various probability distributions (including Poisson distribution for count data and binomial distribution for 
> presence-absence data). To go further, GLMMs permits to handle pseudo-replication generated by non-independant 
> repeated observations by defining one or more random effect in your model.
> Here, GLM(M) tools allows you to define ```year``` and/or ```site``` as random effects to take account of temporal 
> and spatial replicas.
> 
{: .details}

In this tutorial, we'll be working on data extracted from the data portal DATRAS (Database of Trawls Surveys) of the 
International Council for the Exploration of the Sea (ICES). After pre-processing to fit the input format of the tools,
we'll see how to calculate Essential Biodiversity Variables and construct properly, easily and with good practice, 
several GLMMs to test the effect of ```year``` and ```site``` on the species richness of each survey studied and 
on the abundance of each species.

> ### Agenda
>
> In this tutorial, we will cover:
>
> 1. Upload and pre-processing of the data
> {:1_toc}
> 2. Compute Essential Biodiversity Variables and create observation unit file
> {:2_toc}
> 3. GLMMs and plots on ```Community``` and ```Species-Population``` metrics
> {:3_toc}
>
{: .agenda}

# Step 0 : Upload and pre-processing of the data
{:.1_toc}

This first step consist of downloading and properly prepare the data to use it in PAMPA toolsuite.

## Data upload

> ### {% icon hands_on %} Hands-on: Data upload
>
> 1. Create a new history for this tutorial and give it a name for you to find it again later if needed
>
>    {% include snippets/create_new_history.md %} 
>    {% include snippets/rename_history.md %}
>
> 2. Import the CSV files from [Zenodo]() or from the shared data library or directly from DATRAS data portal
>    ```
>    LINK
>    ```
>
>    {% include snippets/import_via_link.md %} 
>    {% include snippets/import_from_data_library.md %}
>
>    The files used for this tutorial have been imported from DATRAS data portal in October 2020, those files are 
>    often updated on the data portal. So if you choose to download files directly from DATRAS there may be 
>    some differences with the files on Zenodo and the final results of this tutorial.
>
>    > ### {% icon tip %} Tip : Importing data from DATRAS data portal
>    >
>    > * Go to link : https://datras.ices.dk/Data_products/Download/Download_Data_public.aspx 
>    > * In the field 'Data products' choose 'CPUE per length per area' 
>    > * In the field 'Survey' choose the survey(s) of interest. Here, we chose to use 'EVHOE', 'BITS' 
>    >   and 'SWC-IBTS' surveys, you'll have to repeat the six following steps for each surveys
>    > * In the field 'Quarter(s)' we chose to tick 'All'
>    > * In the field 'Year(s)' we chose to tick 'All'
>    > * In the field 'Area(s)' we chose to tick 'All'
>    > * In the field 'Species' we chose to tick 'Standard species' and 'All'
>    > * Click on 'Submit'
>    > * Tick 'I accept the documents and their associated restrictions and agree to acknowledge the data source'
>    > * Open the Galaxy Upload Manager ({% icon galaxy-upload %} on the top-right of the tool panel) 
>    > * Drop files on the dedicated area or click 'Choose local files'
>    > * Click on 'Start'
>    > * Click on 'Close' to return to Galaxy tools interface
>    {: .tip}
>
> 3. Make sure that each datasets has the following header ```"Survey","Year","Quarter","Area","AphiaID",
>    "Species","LngtClass","CPUE_number_per_hour"``` and that separator are ","
>
> 4. Change datatype CSV to tabular for each data file
>
>    {% include snippets/change_datatype.md datatype="datatypes" %}
>
{: .hands_on}

These three data files gather Catch Per Unit Effort (CPUE) observations, a relative abundance measure mostly used 
to assess exploited fish species populations :
 - EVHOE is a French survey on Southern Atlantic Bottom Trawl, the extracted file available on Zenodo goes from 
   1997 to 2019 and records CPUE for seven exploited fish species
 - BITS is an international survey on Trawl in Baltic sea, the extracted file available on Zenodo goes from 1991
   to 2020 and records CPUE for three exploited fish species
 - SWC-IBTS is a Scottish survey on bottom trawl in the west coast, the extracted file available on Zenodo goes 
   from 1985 to 2010 and records CPUE for six exploited fish species

## Prepare the data

Before starting the preparation of the data, we need to identify which inputs we need for the tools and their format
(same for ```Community``` or ```Species-Population``` analyses except for a few details) : 
 - Input for the tools to compute metrics : Observation data file containing at least ```year``` and ```location``` sampled 
   that might be represented in one field ```observation.unit```, ```species.code``` to name the species sampled, 
   and ```number``` as the abundance value
 - Supplementary input for the tools to compute a GLM(M) analysis : Observation unit data file containing at least
   ```observation.unit``` and ```year```. It might also contain ```habitat```, ```site``` and any other information
   about the location at a given year that seems useful (examples : ```temperature```,```substrate type```,```plant cover```)

> ### {% icon details %} What is the 'observation.unit' field ?
> 
> The 'observation.unit' field is a primary key linking the observation data file and the metrics data file to an observation 
> unit data file. An unique 'observation.unit' identifier represents an unique sampling event at a given year and location.
> Such a file permits to reduce the size of data files, if all those informations are gathered in the same dataframe 
> same informations will be unnecessarily repeated in many lines.
> 
{: .details}

> ### {% icon tip %} The 'observation.unit' nomenclature
> 
> For this workflow, two nomenclatures are advised :
> * The PAMPA file nomenclature : 
>   ```[Two first letters of 'site'][Two last numbers of 'year'][Identifier of the precise sampled location in four numbers]```
>   The fields ```year``` and ```location``` will be automatically infered from this nomenclature in the first tools to 
>   compute metrics
> 
> * The automatic nomenclature : ```['year' in four numbers]_[Identifier of the precise sampled location in any character chain]```
>   This nomenclature will be automatically constructed in the first tools to compute metrics if the input file doesn't already 
>   have a ```observation.unit``` field but only ```year``` and ```location```
> 
> If you already have your own nomenclature for the primary key linking your files, it will remain untouched by the tools but the 
> output data table from the first tools to compute metrics won't contain ```year``` and ```location``` field.
> In this case and if you want to add such informations to your metrics data file, you can use the tool **Join** {% icon tool %} 
> with the metrics data file and the observation unit data file .
> 
{: .tip}

> ### {% icon details %} The difference between fields ```site``` and ```location``` ?
> 
> Those two fields both inform a geographical indication. 
> ```location``` is the finest geographical level and ```site``` may represent a wider geographical range but can also 
> represent the same geographical level as ```location```. This choice is up to your discretion, depending which geographical
> level you want to take into account in your statistical analysis.
> 
{: .details}

### Concatenation of data files

> ### {% icon hands_on %} Hands-on: Data files concatenation
>
> 1. **Concatenate datasets** {% icon tool %} select the three datasets in {% icon param-files %} *"Datasets to concatenate"*
> 
>    {% include snippets/select_multiple_datasets.md %} 
>
> 2. Verify if the three header lines have been included in the concatenate with **Count occurrences of each record** {% icon tool %}
>    and following parameters :
>    - {% icon param-file %} *"from dataset"* : Concatenated data file
>    - {% icon param-select %} *"Count occurrences of values in column(s)"* : `Column: 1`
>    - {% icon param-select %} *"Delimited by"* : `Tab`
>    - {% icon param-select %} *"How should the results be sorted?"* : `By the values being counted`
>
> 3. If the value `Survey` has more than `1` occurrence use **Filter data on any column** {% icon tool %} with following parameters :
>    - {% icon param-file %} *"Filter"* : Concatenated data file
>    - {% icon param-text %} *"With following condition"* : `c1!='Survey'`
>    - {% icon param-text %} *"Number of header lines to skip"* : `1`
>
> 4. Make sure there is only one header line left with **Count occurrences of each record** {% icon tool %} and following parameters :
>    - {% icon param-file %} *"from dataset"* : Filtered data file
>    - {% icon param-select %} *"Count occurrences of values in column(s)"* : `Column: 1`
>    - {% icon param-select %} *"Delimited by"* : `Tab`
>    - {% icon param-select %} *"How should the results be sorted?"* : `By the values being counted`
>
>    The value `Survey` must have only `1` occurrence
>
{: .hands_on}

### Formating of data files

Here we have data from the three surveys in one datatable with the following fields ```"Survey","Year","Quarter","Area", "AphiaID","Species","LngtClass","CPUE_number_per_hour"```. We now need to define the proper columns needed for the 
PAMPA workflow considering there is no primary key close to ```observation.unit``` :
 - ```location``` from ```Area``` associated with ```Survey``` to avoid mixing up areas from different surveys
 - ```year``` from ```Year```
 - ```species.code``` from ```Species```
 - ```number``` from ```CPUE_number_per_hour```

This concatenated data file containing the three surveys will be used to create the observation unit data file and 
compute ```Community``` metrics as the ```Community``` analysis can be automatically processed on each survey separately. 
However, this file can't be used as it is to compute ```Species-Population``` metrics as it may contain data for several 
populations of the same species that do not interact with each other. Therefore, we have to work on the three original tabular 
files for the ```Species-Population``` analysis. 
In order to avoid mixing up the four data files, we'll add a tag to each file : 
 - tag `#Concatenate` to the concatenated and filtered file containing the three surveys
 - tag `#EVHOE` to the tabular data file of the EVHOE survey
 - tag `#BITS` to the tabular data file of the BITS survey
 - tag `#SWCIBTS` to the tabular data file of the SWC-IBTS survey

{% include snippets/add_tag.md %}

Hence, the following manipulations will be applied not only on concatenated data file but also on the three original tabular
files used for the concatenation.

{% include snippets/select_multiple_datasets.md %}

> ### {% icon hands_on %} Hands-on: Create the location field
>
> 1. **Column Regex Find And Replace** {% icon tool %} with following parameters : 
>    - {% icon param-files %} *"Select cells from"* : Filtered data file ; three tabular data files
>    - {% icon param-select %} *"using column"* : `Column: 1`
>    - {% icon param-repeat %} Click *"+ Insert Check"* : 
>         - {% icon param-text %} *"Find Regex"* : `([A-Z]{3}[A-Z]+)` 
>         - {% icon param-text %} *"Replacement"* : `\1-`
>
>    > ### {% icon question %} Question
>    > What did we do with this tool ?
>    > > ### {% icon solution %} Solution
>    > > We searched for every character chain matching the extended regular expression (Regex) `([A-Z]{3}[A-Z]+)` 
>    > > in the first column ```Survey``` meaning every character chain of 4 or more capital letters. With the replacement `\1-`
>    > > we indicated to add `-` after the bracketed character chain. This manipulation is made in order to merge 
>    > > properly columns ```Area``` and ```Survey``` with a separation.
>    > {: .solution}
>    {: .question}
>
> 2. **Merge Columns** {% icon tool %} with following parameters :
>    - {% icon param-files %} *"Select data"* : four Column Regex Find And Replace data files
>    - {% icon param-select %} *"Merge column"* : `Column: 1`
>    - {% icon param-select %} *"with column"* : `Column: 4`
>
>    Check if the output data files has a supplementary column called ```SurveyArea``` and values with the survey ID and area ID
>    separated with `-`.
{: .hands_on}

> ### {% icon hands_on %} Hands-on: Change column names
>
> **Regex Find And Replace** {% icon tool %} with following parameters :
>    - {% icon param-files %} *"Select lines from"* : four Merged data files
>    - {% icon param-repeat %} Click *"+ Insert Check"* : 
>         - {% icon param-text %} *"Find Regex"* : `Year` 
>         - {% icon param-text %} *"Replacement"* : `year`
>    - {% icon param-repeat %} Click *"+ Insert Check"* : 
>         - {% icon param-text %} *"Find Regex"* : `Species` 
>         - {% icon param-text %} *"Replacement"* : `species.code`
>    - {% icon param-repeat %} Click *"+ Insert Check"* : 
>         - {% icon param-text %} *"Find Regex"* : `CPUE_number_per_hour` 
>         - {% icon param-text %} *"Replacement"* : `number`
>    - {% icon param-repeat %} Click *"+ Insert Check"* : 
>         - {% icon param-text %} *"Find Regex"* : `SurveyArea` 
>         - {% icon param-text %} *"Replacement"* : `location`
>
> Check if the columns are corresponding to ```"Survey","year","Quarter","Area","AphiaID","species.code","LngtClass",
> "number","location"``` (pay attention to case).
>
{: .hands_on}

> ### {% icon details %} Other tools that could be useful to pre-process data
> 
> **Add input name as column** {% icon tool %} ; **Compute an expression on every row** {% icon tool %} 
> ; **Table Compute** {% icon tool %} ; **Join two files on column allowing a small difference** {% icon tool %}
> ; **Join two files** {% icon tool %} ; **Filter Tabular** {% icon tool %} ; **melt** {% icon tool %}
> ; **Unfold columns from a table** {% icon tool %} ; **Remove columns by heading** {% icon tool %} 
> ; **Cut columns from a table** {% icon tool %} ; **Filter data on any column using simple expressions** {% icon tool %}
> ; **Transpose rows/columns in a tabular file** {% icon tool %} ; ...
> 
{: .details}

# Step 1 : Compute Essential Biodiversity Variables and create observation unit file
{:.2_toc}

In this part of the tutorial, we're starting to use tools from the PAMPA toolsuite to compute ```Community``` 
and ```Species-Population``` Essential Biodiversity Variables. Then, we'll be creating the observation unit file.

## Compute ```Community``` and ```Species-Population``` metrics

> ### {% icon hands_on %} Hands-on: Compute ```Community``` metrics
>
> 1. **Calculate community metrics** {% icon tool %} with following parameters : 
>    - {% icon param-file %} *"Input file"* : `#Concatenate` Regex Find And Replace data file
>    - {% icon param-select %} *"Choose the community metrics you want to compute"* : `All`
>
>    Check if the output ```Community``` metrics data file has the following column names ```"year","location","number",
>    "species.richness","simpson","simpson.l","shannon","pielou","hill","observation.unit"```.
>
> 2. Tag the output ```Community``` metrics with `#Community`
>
{: .hands_on}

> ### {% icon hands_on %} Hands-on: Compute ```Species-Population``` metrics
>
> **Calculate presence absence table** {% icon tool %} with following parameters : 
>  - {% icon param-files %} *"Input file"* : `#EVHOE`, `#BITS` and `#SWCIBTS` Regex Find And Replace data files
>
> Check if the outputs ```Species-Population``` metrics data files has the following column names ```"year","location",
> "species.code","number","pres.abs","observation.unit"```.
>
{: .hands_on}

## Creation of the observation unit file

Now that we have the ```observation.unit``` fields automatically created in the ```Community``` and ```Species-Population``` 
metrics files we can create the observation unit file from the `#Concatenate` Regex Find And Replace data file with the same 
nomenclature for the ```observation.unit``` fields, namely : 
```['year' in four numbers]_[Identifier of the precise sampled location in any character chain]```

> ### {% icon hands_on %} Hands-on: Create the observation unit file
>
> 1. **Column Regex Find And Replace** {% icon tool %} with following parameters : 
>    - {% icon param-files %} *"Select cells from"* : `#Concatenate` Regex Find And Replace data file
>    - {% icon param-select %} *"using column"* : `Column: 2`
>    - {% icon param-repeat %} Click *"+ Insert Check"* : 
>         - {% icon param-text %} *"Find Regex"* : `([0-9]{4})` 
>         - {% icon param-text %} *"Replacement"* : `\1_`
>
> 2. **Merge Columns** {% icon tool %} with following parameters :
>    - {% icon param-files %} *"Select data"* : `#Concatenate` Column Regex Find And Replace data file
>    - {% icon param-select %} *"Merge column"* : `Column: 2`
>    - {% icon param-select %} *"with column"* : `Column: 9`
>
>    Check if the output data file has a supplementary column called ```yearlocation``` and values with the year and location ID
>    separated with `_`.
>
> 3. **Column Regex Find And Replace** {% icon tool %} with following parameters : 
>    - {% icon param-files %} *"Select cells from"* : `#Concatenate` Merge Columns data file
>    - {% icon param-select %} *"using column"* : `Column: 2`
>    - {% icon param-repeat %} Click *"+ Insert Check"* : 
>         - {% icon param-text %} *"Find Regex"* : `([0-9]{4})_` 
>         - {% icon param-text %} *"Replacement"* : `\1`
> 
>    > ### {% icon question %} Question
>    > What did we do with this tool ?
>    > > ### {% icon solution %} Solution
>    > > We removed the `_` we added in the first part of this "Hands_on" from the values of the second column 
>    > > ```year``` to get proper numeric values back.
>    > {: .solution}
>    {: .question}
>
> 4. **Advanced Cut** {% icon tool %} with following parameters :
>    - {% icon param-files %} *"File to cut"* : `#Concatenate` Column Regex Find And Replace data file
>    - {% icon param-select %} *"Operation"* : `Keep`
>    - {% icon param-select %} *"Delimited by"* : `Tab`
>    - {% icon param-select %} *"Cut by"* : `fields`
>      - {% icon param-select %} *"List of Fields"* : `Column: 1` `Column: 2` `Column: 3` `Column: 4` `Column: 9` `Column: 10`
>
>    Check if the output `#Concatenate` Advanced Cut data file has the following column names ```"Survey","year","Quarter",
>    "Area","location","yearlocation"```.
>
> 5. **Sort data in ascending or descending order** {% icon tool %} in the "Text Manipulation" 
>    tool collection with following parameters :
>    - {% icon param-files %} *"Sort Query"* : `#Concatenate` Advanced Cut data file
>    - {% icon param-text %} *"Number of header lines"* : `1`
>    - *"1 : Column selections"*
>      - {% icon param-select %} *"on column"* : `Column: 6`
>      - {% icon param-check %} *"in"* : `Ascending order`
>      - {% icon param-check %} *"Flavor"* : `Alphabetical sort`
>      - {% icon param-select %} *"Output unique values"* : `Yes`
>      - {% icon param-select %} *"Ignore case"* : `No`
>
>    The output `#Concatenate` Sort data file must have fewer lines than the `#Concatenate` Advanced Cut data file.
>
> 6. **Regex Find And Replace** {% icon tool %} with following parameters :
>    - {% icon param-files %} *"Select lines from"* : `#Concatenate` Sort data file
>    - {% icon param-repeat %} Click *"+ Insert Check"* : 
>         - {% icon param-text %} *"Find Regex"* : `yearlocation` 
>         - {% icon param-text %} *"Replacement"* : `observation.unit`
>    - {% icon param-repeat %} Click *"+ Insert Check"* : 
>         - {% icon param-text %} *"Find Regex"* : `location` 
>         - {% icon param-text %} *"Replacement"* : `site`
>
>    Here, we define ```site``` represent the same geographical level as ```location```.
>
> 7. Tag the output `#Concatenate` Regex Find And Replace data file with `#unitobs`
>
>    Check if the output `#Concatenate #unitobs` data file has the following column names ```"Survey","year","Quarter",
>    "Area","site","observation.unit"```.
>
{: .hands_on}

# Step 2 & 3 : GLMMs and plots on ```Community``` and ```Species-Population``` metrics
{:.3_toc}

Now that we have all our input files for GLM(M) tools ready, we can start computing statistical models to make 
conclusions on our surveys. But before starting to use the **Compute GLM on community data** {% icon tool %}
and the **Compute GLM on population data** {% icon tool %} tools, we have to think about the model we want to compute. 
The tools allows to test the effects `Year`, `Site` and/or `Habitat`. As we don't have data about the 
`Habitat` we can't test its effect on the interest metric. It is always better to take a temporal and a geographical
variable into account in an ecological model so we'll test the effect of `Year` and `Site` with a random effect 
of `Site` to take geographical pseudo-replication into account.

## Compute GLMMs and create plots on ```Community``` metrics

For the ```Community``` analysis we have the choice to test the effect of `Year` and `Site` on : 
 - ```Total abundance```
 - or ```Species richness```
 - or ```Simpson index```
 - or ```1 - Simpson index```
 - or ```Shannon index```
 - or ```Pielou index``` 
 - or ```Hill index```

We choose to take ```Species richness``` as the interest variable, namely the quantity of different species 
at a given time and location. This metric is located on the fourth column of the `#Concatenate #Community` 
metrics data file. 
As said above, analyses will be separated by the field ```Survey``` (first column) to avoid bias. 

> ### {% icon hands_on %} Hands-on: Compute GLMM on ```Community``` metrics
>
> 1. **Compute GLM on community data** {% icon tool %} with following parameters : 
>     - {% icon param-file %} *"Input metrics file"* : `#Concatenate #Community` metrics data file
>     - {% icon param-file %} *"Unitobs informations file"* : `#Concatenate #unitobs` data file
>     - {% icon param-select %} *"Interest variable from metrics file"* : `Column: 4`
>     - {% icon param-select %} *"Separation factor of your analysis from unitobs file"* : `Column: 1`
>     - {% icon param-select %} *"Response variables"* : `Year` `Site`
>     - {% icon param-select %} *"Random effect ?"* : `Site`
>     - {% icon param-select %} *"Specify advanced parameters"* : `No, use program defaults`
>
> The three outputs must have three tags `#Concatenate #Community #unitobs` and the first one named 'GLM - results 
> from your community analysis' must contain five lines. This file is formated to have one line per GLM processed,
> it has been formated this way to make GLM results easier to represent and modify if needed. However, this format is 
> often not suited to read directly the results on Galaxy as Galaxy handles better tabular files with a lot of lines
> rather than files with a lot of columns. So, if your file doesn't display properly on Galaxy as a data table, we advise
> to do the following optional step.
>
> 2. (optional) **Transpose rows/columns in a tabular file** {% icon tool %} with `#Concatenate #Community #unitobs` 
>    'GLM - results from your community analysis' data file
>
{: .hands_on}

> ### {% icon details %} Details on advanced parameters of GLM tools
> 
> For now, there are two advanced parameters you can setup in the GLM tools : 
>  - {% icon param-select %} *"Distribution for model"* permits you to choose a probability distribution for your 
>    model. When `Auto` is selected, the tool will automatically set the distribution on `Gaussian`, `Binomial`
>    or `Poisson` depending on the type of your interest variable but it may exist a better probability distribution
>    to fit your model. Mind that if you choose to use a random effect, "quasi" distribution can't be used for your model.
>  - {% icon param-select %} *"GLM object(s) as .Rdata output ?"* permits you to get .Rdata file of your GLM(M)s if you
>    want to use it in your own R console.
> 
{: .details}

### Read and interpret raw GLM outputs



***TODO*** Interpret outputs


> ### {% icon hands_on %} Hands-on: Create plots from ```Community``` metrics file and GLMM results 
>
> **Create a plot from GLM data** {% icon tool %} with following parameters : 
>  - {% icon param-file %} *"Unitobs informations file"* : `#Concatenate #Community #unitobs` 'GLM - results 
>    from your community analysis' data file
>  - {% icon param-file %} *"Input metrics file"* : `#Concatenate #Community` metrics data file
>  - {% icon param-file %} *"Unitobs informations file"* : `#Concatenate #unitobs` data file
>
> The output must be a data collection with four PNG files.
>
{: .hands_on}

***TODO*** Interpret outputs

## Compute GLMMs and create plots on ```Species-Population``` metrics

For the ```Species-Population``` analysis we have the choice to test the effect of `Year` and `Site` on : 
 - ```Abundance```
 - or ```Presence-absence```

We choose to take ```Abundance``` as the interest variable, namely the quantity of different species 
at a given time and location. This metric is located on the fourth column of the `#EVHOE`, `#BITS` and `#SWCIBTS`
```Species-Population``` metrics data file. 

> ### {% icon hands_on %} Hands-on: Compute GLMM on ```Community``` metrics
>
> **Compute GLM on population data** {% icon tool %} with following parameters : 
>  - {% icon param-files %} *"Input metrics file"* : `#EVHOE`, `#BITS` and `#SWCIBTS` metrics data files
>  - {% icon param-file %} *"Unitobs informations file"* : `#Concatenate #unitobs` data file
>  - {% icon param-select %} *"Interest variable from metrics file"* : `Column: 4`
>  - {% icon param-select %} *"Response variables"* : `Year` `Site`
>  - {% icon param-select %} *"Random effect ?"* : `Site`
>  - {% icon param-select %} *"Specify advanced parameters"* : `No, use program defaults`
>
> The three outputs for each survey must have three tags `#Concatenate #SurveyID #unitobs`. 
> The first files of each survey named 'GLM - results from your community analysis' must contain one line 
> per GLM processed, it has been formated this way to make GLM results easier to represent and modify if needed.
> However, this format is often not suited to read directly the results on Galaxy as Galaxy handles better tabular 
> files with a lot of lines rather than files with a lot of columns. So, if your files doesn't display properly
> on Galaxy as a data table, we advise to do the following optional step.
>
> 2. (optional) **Transpose rows/columns in a tabular file** {% icon tool %} with `#Concatenate #EVHOE #unitobs` 
>    `#Concatenate #BITS #unitobs` `#Concatenate #SWCIBTS #unitobs` 'GLM - results from your community analysis' data file
>
{: .hands_on}

***TODO*** Details on advanced parameters

### Read and interpret raw GLM outputs

***TODO*** Interpret outputs


> ### {% icon hands_on %} Hands-on: Create plots from ```Community``` metrics file and GLMM results 
>
> **Create a plot from GLM data** {% icon tool %} with following parameters : 
>  - {% icon param-files %} *"Unitobs informations file"* : `#Concatenate #EVHOE #unitobs` `#Concatenate #BITS #unitobs`
>    `#Concatenate #SWCIBTS #unitobs` 'GLM - results from your community analysis' data files
>  - {% icon param-files %} *"Input metrics file"* : `#EVHOE`, `#BITS` and `#SWCIBTS` metrics data files 
>  - {% icon param-file %} *"Unitobs informations file"* : `#Concatenate #unitobs` data file
>
> The three outputs must be a data collection.
>
{: .hands_on}

### 

***TODO*** Interpret outputs

# Conclusion
{:.no_toc}

Sum up the tutorial and the key takeaways here. We encourage adding an overview image of the
pipeline used.
